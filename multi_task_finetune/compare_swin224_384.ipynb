{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuxin/miniconda3/envs/mmpretrain/lib/python3.11/site-packages/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name swin_base_patch4_window12_384_in22k to current swin_base_patch4_window12_384.ms_in22k.\n",
      "  model = create_fn(\n",
      "/home/yuxin/miniconda3/envs/mmpretrain/lib/python3.11/site-packages/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name swin_base_patch4_window7_224_in22k to current swin_base_patch4_window7_224.ms_in22k.\n",
      "  model = create_fn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import timm\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Load pre-trained Swin Transformer models\n",
    "model_384 = timm.create_model('swin_base_patch4_window12_384_in22k', pretrained=False)\n",
    "model_224 = timm.create_model('swin_base_patch4_window7_224_in22k', pretrained=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SwinTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (layers): Sequential(\n",
       "    (0): SwinTransformerStage(\n",
       "      (downsample): Identity()\n",
       "      (blocks): Sequential(\n",
       "        (0): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.004)\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.004)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): SwinTransformerStage(\n",
       "      (downsample): PatchMerging(\n",
       "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (reduction): Linear(in_features=512, out_features=256, bias=False)\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.009)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.009)\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.013)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.013)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): SwinTransformerStage(\n",
       "      (downsample): PatchMerging(\n",
       "        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (reduction): Linear(in_features=1024, out_features=512, bias=False)\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.017)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.017)\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.022)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.022)\n",
       "        )\n",
       "        (2): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.026)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.026)\n",
       "        )\n",
       "        (3): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.030)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.030)\n",
       "        )\n",
       "        (4): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.035)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.035)\n",
       "        )\n",
       "        (5): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.039)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.039)\n",
       "        )\n",
       "        (6): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.043)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.043)\n",
       "        )\n",
       "        (7): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.048)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.048)\n",
       "        )\n",
       "        (8): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.052)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.052)\n",
       "        )\n",
       "        (9): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.057)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.057)\n",
       "        )\n",
       "        (10): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.061)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.061)\n",
       "        )\n",
       "        (11): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.065)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.065)\n",
       "        )\n",
       "        (12): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.070)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.070)\n",
       "        )\n",
       "        (13): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.074)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.074)\n",
       "        )\n",
       "        (14): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.078)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.078)\n",
       "        )\n",
       "        (15): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.083)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.083)\n",
       "        )\n",
       "        (16): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.087)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.087)\n",
       "        )\n",
       "        (17): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.091)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.091)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): SwinTransformerStage(\n",
       "      (downsample): PatchMerging(\n",
       "        (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (reduction): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.096)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.096)\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.100)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.100)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (head): ClassifierHead(\n",
       "    (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (fc): Linear(in_features=1024, out_features=21841, bias=True)\n",
       "    (flatten): Identity()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_224.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SwinTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (layers): Sequential(\n",
       "    (0): SwinTransformerStage(\n",
       "      (downsample): Identity()\n",
       "      (blocks): Sequential(\n",
       "        (0): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.004)\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.004)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): SwinTransformerStage(\n",
       "      (downsample): PatchMerging(\n",
       "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (reduction): Linear(in_features=512, out_features=256, bias=False)\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.009)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.009)\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.013)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.013)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): SwinTransformerStage(\n",
       "      (downsample): PatchMerging(\n",
       "        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (reduction): Linear(in_features=1024, out_features=512, bias=False)\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.017)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.017)\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.022)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.022)\n",
       "        )\n",
       "        (2): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.026)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.026)\n",
       "        )\n",
       "        (3): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.030)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.030)\n",
       "        )\n",
       "        (4): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.035)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.035)\n",
       "        )\n",
       "        (5): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.039)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.039)\n",
       "        )\n",
       "        (6): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.043)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.043)\n",
       "        )\n",
       "        (7): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.048)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.048)\n",
       "        )\n",
       "        (8): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.052)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.052)\n",
       "        )\n",
       "        (9): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.057)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.057)\n",
       "        )\n",
       "        (10): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.061)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.061)\n",
       "        )\n",
       "        (11): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.065)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.065)\n",
       "        )\n",
       "        (12): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.070)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.070)\n",
       "        )\n",
       "        (13): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.074)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.074)\n",
       "        )\n",
       "        (14): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.078)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.078)\n",
       "        )\n",
       "        (15): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.083)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.083)\n",
       "        )\n",
       "        (16): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.087)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.087)\n",
       "        )\n",
       "        (17): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.091)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.091)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): SwinTransformerStage(\n",
       "      (downsample): PatchMerging(\n",
       "        (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (reduction): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.096)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.096)\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.100)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.100)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (head): ClassifierHead(\n",
       "    (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (fc): Linear(in_features=1024, out_features=21841, bias=True)\n",
       "    (flatten): Identity()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_384.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size mismatch: layers.0.blocks.0.attn.relative_position_bias_table, torch.Size([529, 4])\n",
      "size mismatch: layers.0.blocks.1.attn.relative_position_bias_table, torch.Size([529, 4])\n",
      "size mismatch: layers.1.blocks.0.attn.relative_position_bias_table, torch.Size([529, 8])\n",
      "size mismatch: layers.1.blocks.1.attn.relative_position_bias_table, torch.Size([529, 8])\n",
      "size mismatch: layers.2.blocks.0.attn.relative_position_bias_table, torch.Size([529, 16])\n",
      "size mismatch: layers.2.blocks.1.attn.relative_position_bias_table, torch.Size([529, 16])\n",
      "size mismatch: layers.2.blocks.2.attn.relative_position_bias_table, torch.Size([529, 16])\n",
      "size mismatch: layers.2.blocks.3.attn.relative_position_bias_table, torch.Size([529, 16])\n",
      "size mismatch: layers.2.blocks.4.attn.relative_position_bias_table, torch.Size([529, 16])\n",
      "size mismatch: layers.2.blocks.5.attn.relative_position_bias_table, torch.Size([529, 16])\n",
      "size mismatch: layers.2.blocks.6.attn.relative_position_bias_table, torch.Size([529, 16])\n",
      "size mismatch: layers.2.blocks.7.attn.relative_position_bias_table, torch.Size([529, 16])\n",
      "size mismatch: layers.2.blocks.8.attn.relative_position_bias_table, torch.Size([529, 16])\n",
      "size mismatch: layers.2.blocks.9.attn.relative_position_bias_table, torch.Size([529, 16])\n",
      "size mismatch: layers.2.blocks.10.attn.relative_position_bias_table, torch.Size([529, 16])\n",
      "size mismatch: layers.2.blocks.11.attn.relative_position_bias_table, torch.Size([529, 16])\n",
      "size mismatch: layers.2.blocks.12.attn.relative_position_bias_table, torch.Size([529, 16])\n",
      "size mismatch: layers.2.blocks.13.attn.relative_position_bias_table, torch.Size([529, 16])\n",
      "size mismatch: layers.2.blocks.14.attn.relative_position_bias_table, torch.Size([529, 16])\n",
      "size mismatch: layers.2.blocks.15.attn.relative_position_bias_table, torch.Size([529, 16])\n",
      "size mismatch: layers.2.blocks.16.attn.relative_position_bias_table, torch.Size([529, 16])\n",
      "size mismatch: layers.2.blocks.17.attn.relative_position_bias_table, torch.Size([529, 16])\n",
      "size mismatch: layers.3.blocks.0.attn.relative_position_bias_table, torch.Size([529, 32])\n",
      "size mismatch: layers.3.blocks.1.attn.relative_position_bias_table, torch.Size([529, 32])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model_384.state_dict().items():\n",
    "    if name in model_224.state_dict():\n",
    "        if param.size() == model_224.state_dict()[name].size():\n",
    "            model_224.state_dict()[name].copy_(param)\n",
    "        else:\n",
    "            print('size mismatch: {}, {}'.format(name, param.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = '/mnt/hd0/project_large_files/bca_grading/suqh/swin_multi_task/2023_12_15_18:17:54/model-34.pth'\n",
    "ckpt = torch.load(ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'module.base_model.head.fc.bias' in ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load patch_embed.proj.weight\n",
      "load patch_embed.proj.bias\n",
      "load patch_embed.norm.weight\n",
      "load patch_embed.norm.bias\n",
      "load layers.0.blocks.0.norm1.weight\n",
      "load layers.0.blocks.0.norm1.bias\n",
      "load layers.0.blocks.0.attn.qkv.weight\n",
      "load layers.0.blocks.0.attn.proj.weight\n",
      "load layers.0.blocks.0.attn.proj.bias\n",
      "load layers.0.blocks.0.norm2.weight\n",
      "load layers.0.blocks.0.norm2.bias\n",
      "load layers.0.blocks.0.mlp.fc1.weight\n",
      "load layers.0.blocks.0.mlp.fc1.bias\n",
      "load layers.0.blocks.0.mlp.fc2.weight\n",
      "load layers.0.blocks.0.mlp.fc2.bias\n",
      "load layers.0.blocks.1.norm1.weight\n",
      "load layers.0.blocks.1.norm1.bias\n",
      "load layers.0.blocks.1.attn.qkv.weight\n",
      "load layers.0.blocks.1.attn.proj.weight\n",
      "load layers.0.blocks.1.attn.proj.bias\n",
      "load layers.0.blocks.1.norm2.weight\n",
      "load layers.0.blocks.1.norm2.bias\n",
      "load layers.0.blocks.1.mlp.fc1.weight\n",
      "load layers.0.blocks.1.mlp.fc1.bias\n",
      "load layers.0.blocks.1.mlp.fc2.weight\n",
      "load layers.0.blocks.1.mlp.fc2.bias\n",
      "size mismatch: layers.1.downsample.norm.weight, torch.Size([512])\n",
      "size mismatch: layers.1.downsample.norm.bias, torch.Size([512])\n",
      "load layers.1.downsample.reduction.weight\n",
      "load layers.1.blocks.0.norm1.weight\n",
      "load layers.1.blocks.0.norm1.bias\n",
      "load layers.1.blocks.0.attn.qkv.weight\n",
      "load layers.1.blocks.0.attn.proj.weight\n",
      "load layers.1.blocks.0.attn.proj.bias\n",
      "load layers.1.blocks.0.norm2.weight\n",
      "load layers.1.blocks.0.norm2.bias\n",
      "load layers.1.blocks.0.mlp.fc1.weight\n",
      "load layers.1.blocks.0.mlp.fc1.bias\n",
      "load layers.1.blocks.0.mlp.fc2.weight\n",
      "load layers.1.blocks.0.mlp.fc2.bias\n",
      "load layers.1.blocks.1.norm1.weight\n",
      "load layers.1.blocks.1.norm1.bias\n",
      "load layers.1.blocks.1.attn.qkv.weight\n",
      "load layers.1.blocks.1.attn.proj.weight\n",
      "load layers.1.blocks.1.attn.proj.bias\n",
      "load layers.1.blocks.1.norm2.weight\n",
      "load layers.1.blocks.1.norm2.bias\n",
      "load layers.1.blocks.1.mlp.fc1.weight\n",
      "load layers.1.blocks.1.mlp.fc1.bias\n",
      "load layers.1.blocks.1.mlp.fc2.weight\n",
      "load layers.1.blocks.1.mlp.fc2.bias\n",
      "size mismatch: layers.2.downsample.norm.weight, torch.Size([1024])\n",
      "size mismatch: layers.2.downsample.norm.bias, torch.Size([1024])\n",
      "load layers.2.downsample.reduction.weight\n",
      "load layers.2.blocks.0.norm1.weight\n",
      "load layers.2.blocks.0.norm1.bias\n",
      "load layers.2.blocks.0.attn.qkv.weight\n",
      "load layers.2.blocks.0.attn.proj.weight\n",
      "load layers.2.blocks.0.attn.proj.bias\n",
      "load layers.2.blocks.0.norm2.weight\n",
      "load layers.2.blocks.0.norm2.bias\n",
      "load layers.2.blocks.0.mlp.fc1.weight\n",
      "load layers.2.blocks.0.mlp.fc1.bias\n",
      "load layers.2.blocks.0.mlp.fc2.weight\n",
      "load layers.2.blocks.0.mlp.fc2.bias\n",
      "load layers.2.blocks.1.norm1.weight\n",
      "load layers.2.blocks.1.norm1.bias\n",
      "load layers.2.blocks.1.attn.qkv.weight\n",
      "load layers.2.blocks.1.attn.proj.weight\n",
      "load layers.2.blocks.1.attn.proj.bias\n",
      "load layers.2.blocks.1.norm2.weight\n",
      "load layers.2.blocks.1.norm2.bias\n",
      "load layers.2.blocks.1.mlp.fc1.weight\n",
      "load layers.2.blocks.1.mlp.fc1.bias\n",
      "load layers.2.blocks.1.mlp.fc2.weight\n",
      "load layers.2.blocks.1.mlp.fc2.bias\n",
      "load layers.2.blocks.2.norm1.weight\n",
      "load layers.2.blocks.2.norm1.bias\n",
      "load layers.2.blocks.2.attn.qkv.weight\n",
      "load layers.2.blocks.2.attn.proj.weight\n",
      "load layers.2.blocks.2.attn.proj.bias\n",
      "load layers.2.blocks.2.norm2.weight\n",
      "load layers.2.blocks.2.norm2.bias\n",
      "load layers.2.blocks.2.mlp.fc1.weight\n",
      "load layers.2.blocks.2.mlp.fc1.bias\n",
      "load layers.2.blocks.2.mlp.fc2.weight\n",
      "load layers.2.blocks.2.mlp.fc2.bias\n",
      "load layers.2.blocks.3.norm1.weight\n",
      "load layers.2.blocks.3.norm1.bias\n",
      "load layers.2.blocks.3.attn.qkv.weight\n",
      "load layers.2.blocks.3.attn.proj.weight\n",
      "load layers.2.blocks.3.attn.proj.bias\n",
      "load layers.2.blocks.3.norm2.weight\n",
      "load layers.2.blocks.3.norm2.bias\n",
      "load layers.2.blocks.3.mlp.fc1.weight\n",
      "load layers.2.blocks.3.mlp.fc1.bias\n",
      "load layers.2.blocks.3.mlp.fc2.weight\n",
      "load layers.2.blocks.3.mlp.fc2.bias\n",
      "load layers.2.blocks.4.norm1.weight\n",
      "load layers.2.blocks.4.norm1.bias\n",
      "load layers.2.blocks.4.attn.qkv.weight\n",
      "load layers.2.blocks.4.attn.proj.weight\n",
      "load layers.2.blocks.4.attn.proj.bias\n",
      "load layers.2.blocks.4.norm2.weight\n",
      "load layers.2.blocks.4.norm2.bias\n",
      "load layers.2.blocks.4.mlp.fc1.weight\n",
      "load layers.2.blocks.4.mlp.fc1.bias\n",
      "load layers.2.blocks.4.mlp.fc2.weight\n",
      "load layers.2.blocks.4.mlp.fc2.bias\n",
      "load layers.2.blocks.5.norm1.weight\n",
      "load layers.2.blocks.5.norm1.bias\n",
      "load layers.2.blocks.5.attn.qkv.weight\n",
      "load layers.2.blocks.5.attn.proj.weight\n",
      "load layers.2.blocks.5.attn.proj.bias\n",
      "load layers.2.blocks.5.norm2.weight\n",
      "load layers.2.blocks.5.norm2.bias\n",
      "load layers.2.blocks.5.mlp.fc1.weight\n",
      "load layers.2.blocks.5.mlp.fc1.bias\n",
      "load layers.2.blocks.5.mlp.fc2.weight\n",
      "load layers.2.blocks.5.mlp.fc2.bias\n",
      "load layers.2.blocks.6.norm1.weight\n",
      "load layers.2.blocks.6.norm1.bias\n",
      "load layers.2.blocks.6.attn.qkv.weight\n",
      "load layers.2.blocks.6.attn.proj.weight\n",
      "load layers.2.blocks.6.attn.proj.bias\n",
      "load layers.2.blocks.6.norm2.weight\n",
      "load layers.2.blocks.6.norm2.bias\n",
      "load layers.2.blocks.6.mlp.fc1.weight\n",
      "load layers.2.blocks.6.mlp.fc1.bias\n",
      "load layers.2.blocks.6.mlp.fc2.weight\n",
      "load layers.2.blocks.6.mlp.fc2.bias\n",
      "load layers.2.blocks.7.norm1.weight\n",
      "load layers.2.blocks.7.norm1.bias\n",
      "load layers.2.blocks.7.attn.qkv.weight\n",
      "load layers.2.blocks.7.attn.proj.weight\n",
      "load layers.2.blocks.7.attn.proj.bias\n",
      "load layers.2.blocks.7.norm2.weight\n",
      "load layers.2.blocks.7.norm2.bias\n",
      "load layers.2.blocks.7.mlp.fc1.weight\n",
      "load layers.2.blocks.7.mlp.fc1.bias\n",
      "load layers.2.blocks.7.mlp.fc2.weight\n",
      "load layers.2.blocks.7.mlp.fc2.bias\n",
      "load layers.2.blocks.8.norm1.weight\n",
      "load layers.2.blocks.8.norm1.bias\n",
      "load layers.2.blocks.8.attn.qkv.weight\n",
      "load layers.2.blocks.8.attn.proj.weight\n",
      "load layers.2.blocks.8.attn.proj.bias\n",
      "load layers.2.blocks.8.norm2.weight\n",
      "load layers.2.blocks.8.norm2.bias\n",
      "load layers.2.blocks.8.mlp.fc1.weight\n",
      "load layers.2.blocks.8.mlp.fc1.bias\n",
      "load layers.2.blocks.8.mlp.fc2.weight\n",
      "load layers.2.blocks.8.mlp.fc2.bias\n",
      "load layers.2.blocks.9.norm1.weight\n",
      "load layers.2.blocks.9.norm1.bias\n",
      "load layers.2.blocks.9.attn.qkv.weight\n",
      "load layers.2.blocks.9.attn.proj.weight\n",
      "load layers.2.blocks.9.attn.proj.bias\n",
      "load layers.2.blocks.9.norm2.weight\n",
      "load layers.2.blocks.9.norm2.bias\n",
      "load layers.2.blocks.9.mlp.fc1.weight\n",
      "load layers.2.blocks.9.mlp.fc1.bias\n",
      "load layers.2.blocks.9.mlp.fc2.weight\n",
      "load layers.2.blocks.9.mlp.fc2.bias\n",
      "load layers.2.blocks.10.norm1.weight\n",
      "load layers.2.blocks.10.norm1.bias\n",
      "load layers.2.blocks.10.attn.qkv.weight\n",
      "load layers.2.blocks.10.attn.proj.weight\n",
      "load layers.2.blocks.10.attn.proj.bias\n",
      "load layers.2.blocks.10.norm2.weight\n",
      "load layers.2.blocks.10.norm2.bias\n",
      "load layers.2.blocks.10.mlp.fc1.weight\n",
      "load layers.2.blocks.10.mlp.fc1.bias\n",
      "load layers.2.blocks.10.mlp.fc2.weight\n",
      "load layers.2.blocks.10.mlp.fc2.bias\n",
      "load layers.2.blocks.11.norm1.weight\n",
      "load layers.2.blocks.11.norm1.bias\n",
      "load layers.2.blocks.11.attn.qkv.weight\n",
      "load layers.2.blocks.11.attn.proj.weight\n",
      "load layers.2.blocks.11.attn.proj.bias\n",
      "load layers.2.blocks.11.norm2.weight\n",
      "load layers.2.blocks.11.norm2.bias\n",
      "load layers.2.blocks.11.mlp.fc1.weight\n",
      "load layers.2.blocks.11.mlp.fc1.bias\n",
      "load layers.2.blocks.11.mlp.fc2.weight\n",
      "load layers.2.blocks.11.mlp.fc2.bias\n",
      "load layers.2.blocks.12.norm1.weight\n",
      "load layers.2.blocks.12.norm1.bias\n",
      "load layers.2.blocks.12.attn.qkv.weight\n",
      "load layers.2.blocks.12.attn.proj.weight\n",
      "load layers.2.blocks.12.attn.proj.bias\n",
      "load layers.2.blocks.12.norm2.weight\n",
      "load layers.2.blocks.12.norm2.bias\n",
      "load layers.2.blocks.12.mlp.fc1.weight\n",
      "load layers.2.blocks.12.mlp.fc1.bias\n",
      "load layers.2.blocks.12.mlp.fc2.weight\n",
      "load layers.2.blocks.12.mlp.fc2.bias\n",
      "load layers.2.blocks.13.norm1.weight\n",
      "load layers.2.blocks.13.norm1.bias\n",
      "load layers.2.blocks.13.attn.qkv.weight\n",
      "load layers.2.blocks.13.attn.proj.weight\n",
      "load layers.2.blocks.13.attn.proj.bias\n",
      "load layers.2.blocks.13.norm2.weight\n",
      "load layers.2.blocks.13.norm2.bias\n",
      "load layers.2.blocks.13.mlp.fc1.weight\n",
      "load layers.2.blocks.13.mlp.fc1.bias\n",
      "load layers.2.blocks.13.mlp.fc2.weight\n",
      "load layers.2.blocks.13.mlp.fc2.bias\n",
      "load layers.2.blocks.14.norm1.weight\n",
      "load layers.2.blocks.14.norm1.bias\n",
      "load layers.2.blocks.14.attn.qkv.weight\n",
      "load layers.2.blocks.14.attn.proj.weight\n",
      "load layers.2.blocks.14.attn.proj.bias\n",
      "load layers.2.blocks.14.norm2.weight\n",
      "load layers.2.blocks.14.norm2.bias\n",
      "load layers.2.blocks.14.mlp.fc1.weight\n",
      "load layers.2.blocks.14.mlp.fc1.bias\n",
      "load layers.2.blocks.14.mlp.fc2.weight\n",
      "load layers.2.blocks.14.mlp.fc2.bias\n",
      "load layers.2.blocks.15.norm1.weight\n",
      "load layers.2.blocks.15.norm1.bias\n",
      "load layers.2.blocks.15.attn.qkv.weight\n",
      "load layers.2.blocks.15.attn.proj.weight\n",
      "load layers.2.blocks.15.attn.proj.bias\n",
      "load layers.2.blocks.15.norm2.weight\n",
      "load layers.2.blocks.15.norm2.bias\n",
      "load layers.2.blocks.15.mlp.fc1.weight\n",
      "load layers.2.blocks.15.mlp.fc1.bias\n",
      "load layers.2.blocks.15.mlp.fc2.weight\n",
      "load layers.2.blocks.15.mlp.fc2.bias\n",
      "load layers.2.blocks.16.norm1.weight\n",
      "load layers.2.blocks.16.norm1.bias\n",
      "load layers.2.blocks.16.attn.qkv.weight\n",
      "load layers.2.blocks.16.attn.proj.weight\n",
      "load layers.2.blocks.16.attn.proj.bias\n",
      "load layers.2.blocks.16.norm2.weight\n",
      "load layers.2.blocks.16.norm2.bias\n",
      "load layers.2.blocks.16.mlp.fc1.weight\n",
      "load layers.2.blocks.16.mlp.fc1.bias\n",
      "load layers.2.blocks.16.mlp.fc2.weight\n",
      "load layers.2.blocks.16.mlp.fc2.bias\n",
      "load layers.2.blocks.17.norm1.weight\n",
      "load layers.2.blocks.17.norm1.bias\n",
      "load layers.2.blocks.17.attn.qkv.weight\n",
      "load layers.2.blocks.17.attn.proj.weight\n",
      "load layers.2.blocks.17.attn.proj.bias\n",
      "load layers.2.blocks.17.norm2.weight\n",
      "load layers.2.blocks.17.norm2.bias\n",
      "load layers.2.blocks.17.mlp.fc1.weight\n",
      "load layers.2.blocks.17.mlp.fc1.bias\n",
      "load layers.2.blocks.17.mlp.fc2.weight\n",
      "load layers.2.blocks.17.mlp.fc2.bias\n",
      "size mismatch: layers.3.downsample.norm.weight, torch.Size([2048])\n",
      "size mismatch: layers.3.downsample.norm.bias, torch.Size([2048])\n",
      "load layers.3.downsample.reduction.weight\n",
      "load layers.3.blocks.0.norm1.weight\n",
      "load layers.3.blocks.0.norm1.bias\n",
      "load layers.3.blocks.0.attn.qkv.weight\n",
      "load layers.3.blocks.0.attn.proj.weight\n",
      "load layers.3.blocks.0.attn.proj.bias\n",
      "load layers.3.blocks.0.norm2.weight\n",
      "load layers.3.blocks.0.norm2.bias\n",
      "load layers.3.blocks.0.mlp.fc1.weight\n",
      "load layers.3.blocks.0.mlp.fc1.bias\n",
      "load layers.3.blocks.0.mlp.fc2.weight\n",
      "load layers.3.blocks.0.mlp.fc2.bias\n",
      "load layers.3.blocks.1.norm1.weight\n",
      "load layers.3.blocks.1.norm1.bias\n",
      "load layers.3.blocks.1.attn.qkv.weight\n",
      "load layers.3.blocks.1.attn.proj.weight\n",
      "load layers.3.blocks.1.attn.proj.bias\n",
      "load layers.3.blocks.1.norm2.weight\n",
      "load layers.3.blocks.1.norm2.bias\n",
      "load layers.3.blocks.1.mlp.fc1.weight\n",
      "load layers.3.blocks.1.mlp.fc1.bias\n",
      "load layers.3.blocks.1.mlp.fc2.weight\n",
      "load layers.3.blocks.1.mlp.fc2.bias\n",
      "load norm.weight\n",
      "load norm.bias\n",
      "size mismatch: head.fc.weight, torch.Size([21841, 1024])\n",
      "size mismatch: head.fc.bias, torch.Size([21841])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model_224.state_dict().items():\n",
    "    if f'module.base_model.{name}' in ckpt:\n",
    "        if param.size() == ckpt[f'module.base_model.{name}'].size():\n",
    "            print('load {}'.format(name))\n",
    "            model_224.state_dict()[name].copy_(param)\n",
    "        else:\n",
    "            print('size mismatch: {}, {}'.format(name, param.size()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmpretrain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
