{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import pathlib\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from multiprocessing import Pool\n",
    "# import umap\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix, cohen_kappa_score, accuracy_score\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# import albumentations as A\n",
    "# from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "import time\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "import timm\n",
    "from sam import SAM\n",
    "from apex import amp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "img_size = patch_size = 384\n",
    "data_root = pathlib.Path('/home/yuxin/ssd_data/SUQH_10x336')\n",
    "phases = ['train', 'val', 'test']\n",
    "save_root = pathlib.Path('/mnt/hd0/project_large_files/bca_grading/suqh/swin_multi_task')\n",
    "save_root.mkdir(exist_ok=True)\n",
    "data_trans = {\n",
    "    \"train\": transforms.Compose([\n",
    "                                #  transforms.RandomResizedCrop(img_size),\n",
    "                                # transforms.RandomCrop(img_size*4, padding_mode='reflect', pad_if_needed=True),\n",
    "                                # transforms.CenterCrop(img_size),\n",
    "                                #  transforms.CenterCrop(img_size*4),\n",
    "                                # transforms.Resize(224),\n",
    "                                # RandomPatchCrop(img_size*4),\n",
    "                                transforms.Resize(img_size),\n",
    "                                transforms.RandomHorizontalFlip(),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),\n",
    "    \"test\": transforms.Compose([\n",
    "                                # transforms.Resize(int(img_size * 1.143)),\n",
    "                                # transforms.CenterCrop(img_size*4),\n",
    "                                transforms.Resize(img_size),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])}\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "class MyDataSet(Dataset):\n",
    "    \"\"\"自定义数据集\"\"\"\n",
    "\n",
    "    # def __init__(self, images_path: list, images_class: list, images_ncl: list, images_epi: list, images_tub: list, images_mit: list, transform=None):\n",
    "    \n",
    "    def get_type_grade(self, path):\n",
    "        type_grade = path.parent.name\n",
    "        if type_grade == 'normal':\n",
    "            return 0,0\n",
    "        elif type_grade == 'tis-1':\n",
    "            return 1,0\n",
    "        elif type_grade == 'tis-2':\n",
    "            return 1,1\n",
    "        elif type_grade == 'tis-3':\n",
    "            return 1,2\n",
    "        elif type_grade == 'it-1':\n",
    "            return 2,0\n",
    "        elif type_grade == 'it-2':\n",
    "            return 2,1\n",
    "        elif type_grade == 'it-3':\n",
    "            return 2,2\n",
    "    \n",
    "    def __init__(self, data_root:pathlib, phase:str, transform=None):\n",
    "        self.images_path = list((data_root/phase).glob('**/*.jpg'))\n",
    "        self.images_class = [self.get_type_grade(path) for path in self.images_path]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_path)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        img = Image.open(self.images_path[item])\n",
    "        # RGB为彩色图片，L为灰度图片\n",
    "        if img.mode != 'RGB':\n",
    "            raise ValueError(\"image: {} isn't RGB mode.\".format(self.images_path[item]))\n",
    "        label = self.images_class[item]\n",
    "\n",
    "        # img_full, img_name = os.path.split(self.images_path[item])\n",
    "        # img_n, ext = os.path.splitext(img_name)\n",
    "\n",
    "        # seed = np.random.randint(2147483647)\n",
    "        if self.transform is not None:\n",
    "            # torch.manual_seed(seed)\n",
    "            # torch.cuda.manual_seed(seed)\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        # 官方实现的default_collate可以参考\n",
    "        # https://github.com/pytorch/pytorch/blob/67b7e751e6b5931a9f45274653f4f653a4e6cdf6/torch/utils/data/_utils/collate.py\n",
    "        images, labels = tuple(zip(*batch))\n",
    "\n",
    "        images = torch.stack(images, dim=0)\n",
    "        labels = torch.as_tensor(labels)\n",
    "\n",
    "        return images, labels\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "num_classes = [3,3]\n",
    "\n",
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self, base_model_name, num_classes):\n",
    "        super(MultiTaskModel, self).__init__()\n",
    "        ckpt_dict = {\n",
    "            'swin_base_patch4_window12_384_in22k': dict(file='/home/yuxin/Downloads/swin_base_patch4_window12_384_22k.pth'),\n",
    "        }\n",
    "        self.base_model = timm.create_model(base_model_name, pretrained=True, pretrained_cfg_overlay=ckpt_dict[base_model_name])\n",
    "        # print(self.base_model)\n",
    "        self.num_features = self.base_model.num_features\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.base_model.feature = nn.Sequential(\n",
    "            self.base_model.head.global_pool,\n",
    "            self.base_model.head.drop,\n",
    "        )\n",
    "        if isinstance(num_classes, list):\n",
    "            self.heads = nn.ModuleList([nn.Linear(self.num_features, num_class) for num_class in num_classes])\n",
    "        else:\n",
    "            self.head = self.base_model.head\n",
    "            self.head.fc = nn.Linear(self.num_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the base model\n",
    "        x = self.base_model.forward_features(x)\n",
    "        x = self.base_model.feature(x)\n",
    "        # print(x.shape)\n",
    "\n",
    "        if isinstance(self.num_classes, list):\n",
    "            x = [head(x) for head in self.heads]\n",
    "        else:\n",
    "            x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'swin_base_patch4_window12_384_in22k'\n",
    "\n",
    "model = MultiTaskModel(model_name, num_classes)\n",
    "\n",
    "device = \"cuda\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=16\n",
    "# 实例化训练数据集\n",
    "train_dataset = MyDataSet(data_root=data_root,\n",
    "                            phase='train',\n",
    "                            transform=data_trans[\"train\"])\n",
    "\n",
    "# 实例化验证数据集\n",
    "val_dataset = MyDataSet(data_root=data_root,\n",
    "                        phase='val',\n",
    "                        transform=data_trans[\"test\"])\n",
    "\n",
    "batch_size = bs\n",
    "nw = min([os.cpu_count(), batch_size if batch_size > 1 else 0, 16])  # number of workers\n",
    "print('Using {} dataloader workers every process'.format(nw))\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True,\n",
    "                                            pin_memory=True,\n",
    "                                            num_workers=nw,\n",
    "                                            collate_fn=train_dataset.collate_fn)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                            batch_size=batch_size//2,\n",
    "                                            shuffle=False,\n",
    "                                            pin_memory=True,\n",
    "                                            num_workers=nw,\n",
    "                                            collate_fn=val_dataset.collate_fn)\n",
    "\n",
    "# test set\n",
    "test_dataset = MyDataSet(data_root=data_root, phase='test',\n",
    "                            transform=data_trans['test'])\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                            batch_size=batch_size//2,\n",
    "                                            shuffle=False,\n",
    "                                            pin_memory=True,\n",
    "                                            num_workers=nw,\n",
    "                                            collate_fn=test_dataset.collate_fn)\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "# model\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "model = nn.DataParallel(model)\n",
    "\n",
    "\n",
    "weights_dir = pathlib.Path('/mnt/hd0/project_large_files/bca_grading/suqh/swin_multi_task/2023_12_13_18:12:34')\n",
    "logs_save_path = weights_dir\n",
    "\n",
    "def iterate(model, data_loader, device, optimizer=None, phase='test', epoch='Test'):\n",
    "    if phase == 'train':\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "    else:\n",
    "        model.eval()\n",
    "    \n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    accu_loss = torch.zeros(1).to(device)  # 累计损失\n",
    "    sample_num = 0\n",
    "    tasks = ['type', 'grade']\n",
    "    y_true, y_pred, y_prob = {}, {}, {}\n",
    "    for t in tasks:\n",
    "        y_true[t], y_pred[t], y_prob[t] = [], [], []\n",
    "    data_loader = tqdm(data_loader, file=sys.stdout)\n",
    "    for step, data in enumerate(data_loader):\n",
    "        images, labels = data\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        sample_num += images.shape[0]\n",
    "\n",
    "        outputs = model.forward(images)\n",
    "\n",
    "        losses = []\n",
    "\n",
    "        for i in range(len(outputs)):\n",
    "            task = tasks[i]\n",
    "            output = outputs[i]\n",
    "            pred = torch.max(output, dim=1)[1]\n",
    "            label = labels[:,i]\n",
    "            # print(output.shape, label.shape)\n",
    "            loss = loss_function(output, label)\n",
    "            losses.append(loss)\n",
    "\n",
    "\n",
    "            y_pred[task].extend(pred.tolist())\n",
    "            y_true[task].extend(label.to(device).tolist())\n",
    "            \n",
    "            prob = F.softmax(output, dim=1)\n",
    "            y_prob[task].extend(prob.to(device).tolist())\n",
    "        \n",
    "        loss = sum(losses) / len(losses)\n",
    "        \n",
    "        accu_loss += loss.detach()\n",
    "\n",
    "        # compute acc\n",
    "        acc = []\n",
    "        f1_macro = []\n",
    "        for task in tasks:\n",
    "            acc.append(accuracy_score(y_true[task], y_pred[task]))\n",
    "            f1_macro.append(f1_score(y_true[task], y_pred[task], average='macro'))\n",
    "\n",
    "\n",
    "        data_loader.desc = \"[{} epoch {}] loss: {:.3f}, acc: {:.3f},{:.3f}, f1_marco: {:.3f},{:.3f}\".format(\n",
    "            phase, epoch, accu_loss.item() / (step + 1), acc[0], acc[1], f1_macro[0], f1_macro[1])\n",
    "\n",
    "        if not torch.isfinite(loss):\n",
    "            print('WARNING: non-finite loss, ending training ', loss)\n",
    "            sys.exit(1)\n",
    "        if phase == 'train':\n",
    "            loss.backward()\n",
    "            optimizer.first_step(zero_grad=True)\n",
    "            outputs = model.forward(images)\n",
    "            losses = []\n",
    "            for i in range(len(outputs)):\n",
    "                output = outputs[i]\n",
    "                loss = loss_function(output, labels[:,i])\n",
    "                losses.append(loss)\n",
    "            loss = sum(losses) / len(losses)\n",
    "            loss.backward()\n",
    "            optimizer.second_step(zero_grad=True)\n",
    "            scheduler.step()\n",
    "\n",
    "    return accu_loss.item() / (step + 1), acc, f1_macro, y_true, y_pred, y_prob\n",
    "\n",
    "if os.path.exists(logs_save_path) is False:\n",
    "    os.makedirs(logs_save_path)\n",
    "log_filename = 'test_'+time.strftime(\"%Y_%m_%d_%H:%M:%S\") + '.txt'\n",
    "log_filepath = os.path.join(logs_save_path, log_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_paths  = list(weights_dir.glob('*.pth'))\n",
    "weights_path = weight_paths[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weight_path = weights_path\n",
    "model.load_state_dict(torch.load(model_weight_path, map_location=device))\n",
    "\n",
    "# validate\n",
    "_,_,_,y_true,y_pred,y_prob  = iterate(model=model,\n",
    "            data_loader=test_loader,\n",
    "            device=device,\n",
    "            epoch=weights_path.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_level\n",
    "Acc = {}\n",
    "F1score = {}\n",
    "Auc = {}\n",
    "Kappa = {}\n",
    "Precision = {}\n",
    "Recall = {}\n",
    "# for task in ['type', 'grade']:\n",
    "    \n",
    "#     Acc[task] = accuracy_score(y_true[task], y_pred[task])\n",
    "#     y_true_b = label_binarize(y_true[task], classes=[0, 1, 2])\n",
    "#     Auc[task] = roc_auc_score(y_true_b, y_prob[task], multi_class='ovo', average=None)\n",
    "#     Kappa[task] = cohen_kappa_score(y_true[task], y_pred[task], weights='quadratic')\n",
    "#     Precision[task], Recall[task], F1score[task], _ = precision_recall_fscore_support(y_true[task], y_pred[task])\n",
    "\n",
    "# for type\n",
    "type_y_pred = y_pred['type']\n",
    "type_y_prob = y_prob['type']\n",
    "type_y_true = y_true['type']\n",
    "Acc['type'] = accuracy_score(type_y_true, type_y_pred)\n",
    "type_y_true_b = label_binarize(type_y_true, classes=[0, 1, 2])\n",
    "Auc['type'] = roc_auc_score(type_y_true_b, type_y_prob, multi_class='ovo', average=None)\n",
    "Kappa['type'] = cohen_kappa_score(type_y_true, type_y_pred, weights='quadratic')\n",
    "Precision['type'], Recall['type'], F1score['type'], _ = precision_recall_fscore_support(type_y_true, type_y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tumor_idx = np.where(np.array(type_y_true) != 0)[0]\n",
    "grade_y_pred = np.array(y_pred['grade'])[tumor_idx]\n",
    "grade_y_prob = np.array(y_prob['grade'])[tumor_idx]\n",
    "grade_y_true = np.array(y_true['grade'])[tumor_idx]\n",
    "Acc['grade'] = accuracy_score(grade_y_true, grade_y_pred)\n",
    "grade_y_true_b = label_binarize(grade_y_true, classes=[0, 1, 2])\n",
    "Auc['grade'] = roc_auc_score(grade_y_true_b, grade_y_prob, multi_class='ovo', average=None)\n",
    "Kappa['grade'] = cohen_kappa_score(grade_y_true, grade_y_pred, weights='quadratic')\n",
    "Precision['grade'], Recall['grade'], F1score['grade'], _ = precision_recall_fscore_support(grade_y_true, grade_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_paths.sort(key=lambda x: int(x.stem.split('-')[1]))\n",
    "weight_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmpretrain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
