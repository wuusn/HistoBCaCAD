{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "from PIL import Image\n",
    "import pathlib\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from multiprocessing import Pool\n",
    "# import umap\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix, cohen_kappa_score, accuracy_score\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# import albumentations as A\n",
    "# from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "import time\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "import timm\n",
    "from sam import SAM\n",
    "from apex import amp\n",
    "\n",
    "\n",
    "def copy_self(destination):\n",
    "    # 获取当前脚本的绝对路径\n",
    "    script_path = os.path.realpath(__file__)\n",
    "    \n",
    "    # 拷贝文件到目标位置\n",
    "    shutil.copy2(script_path, destination)\n",
    "\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "num_classes = [3,3]\n",
    "img_size = patch_size = 384\n",
    "\n",
    "data_trans = {\n",
    "    \"train\": transforms.Compose([\n",
    "                                transforms.Resize(img_size),\n",
    "                                transforms.RandomHorizontalFlip(),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),\n",
    "    \"test\": transforms.Compose([\n",
    "                                transforms.Resize(img_size),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])}\n",
    "\n",
    "class MyDataSet(Dataset):\n",
    "    \"\"\"自定义数据集\"\"\"\n",
    "\n",
    "    # def __init__(self, images_path: list, images_class: list, images_ncl: list, images_epi: list, images_tub: list, images_mit: list, transform=None):\n",
    "    \n",
    "    def get_type_grade(self, path):\n",
    "        type_grade = path.parent.name\n",
    "        if type_grade == 'normal':\n",
    "            return 0,0\n",
    "        elif type_grade == 'tis-1':\n",
    "            return 1,0\n",
    "        elif type_grade == 'tis-2':\n",
    "            return 1,1\n",
    "        elif type_grade == 'tis-3':\n",
    "            return 1,2\n",
    "        elif type_grade == 'it-1':\n",
    "            return 2,0\n",
    "        elif type_grade == 'it-2':\n",
    "            return 2,1\n",
    "        elif type_grade == 'it-3':\n",
    "            return 2,2\n",
    "    \n",
    "    def __init__(self, data_root:pathlib, transform=None):\n",
    "        self.images_path = list((data_root).glob('**/*.jpg'))\n",
    "        self.images_class = [self.get_type_grade(path) for path in self.images_path]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_path)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        img = Image.open(self.images_path[item])\n",
    "        # RGB为彩色图片，L为灰度图片\n",
    "        if img.mode != 'RGB':\n",
    "            raise ValueError(\"image: {} isn't RGB mode.\".format(self.images_path[item]))\n",
    "        label = self.images_class[item]\n",
    "\n",
    "        # img_full, img_name = os.path.split(self.images_path[item])\n",
    "        # img_n, ext = os.path.splitext(img_name)\n",
    "\n",
    "        # seed = np.random.randint(2147483647)\n",
    "        if self.transform is not None:\n",
    "            # torch.manual_seed(seed)\n",
    "            # torch.cuda.manual_seed(seed)\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        # 官方实现的default_collate可以参考\n",
    "        # https://github.com/pytorch/pytorch/blob/67b7e751e6b5931a9f45274653f4f653a4e6cdf6/torch/utils/data/_utils/collate.py\n",
    "        images, labels = tuple(zip(*batch))\n",
    "\n",
    "        images = torch.stack(images, dim=0)\n",
    "        labels = torch.as_tensor(labels)\n",
    "\n",
    "        return images, labels\n",
    "SUQH = QDUH = SHSU = MyDataSet\n",
    "\n",
    "class BACH(MyDataSet):\n",
    "    def get_type_grade(self, path):\n",
    "        type_grade = path.parent.name\n",
    "        if type_grade == 'Normal':\n",
    "            return 0,0\n",
    "        elif type_grade == 'InSitu':\n",
    "            return 1,0\n",
    "        elif type_grade == 'Invasive':\n",
    "            return 2,0\n",
    "    \n",
    "    def __init__(self, data_root:pathlib, transform=None):\n",
    "        self.images_path = []\n",
    "        for l in ['Normal', 'InSitu', 'Invasive']:\n",
    "            self.images_path.extend(list((data_root/l).glob('**/*.tif')))\n",
    "        self.images_class = [self.get_type_grade(path) for path in self.images_path]\n",
    "        self.transform = transform\n",
    "\n",
    "class BRACS(MyDataSet):\n",
    "    def get_type_grade(self, path):\n",
    "        type_grade = path.parent.name\n",
    "        if type_grade == '0_N':\n",
    "            return 0,0\n",
    "        elif type_grade == '5_DCIS':\n",
    "            return 1,0\n",
    "        elif type_grade == '6_IC':\n",
    "            return 2,0\n",
    "    \n",
    "    def __init__(self, data_root:pathlib, transform=None):\n",
    "        self.images_path = []\n",
    "        for l in ['0_N', '5_DCIS', '6_IC']:\n",
    "            self.images_path.extend(list((data_root).glob(f'*/{l}/*.png')))\n",
    "        self.images_class = [self.get_type_grade(path) for path in self.images_path]\n",
    "        self.transform = transform\n",
    "\n",
    "\n",
    "\n",
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self, base_model_name, num_classes):\n",
    "        super(MultiTaskModel, self).__init__()\n",
    "        ckpt_dict = {\n",
    "            'swin_base_patch4_window12_384_in22k': dict(file='/home/yuxin/Downloads/swin_base_patch4_window12_384_22k.pth'),\n",
    "        }\n",
    "        self.base_model = timm.create_model(base_model_name, pretrained=True, pretrained_cfg_overlay=ckpt_dict[base_model_name])\n",
    "        # print(self.base_model)\n",
    "        self.num_features = self.base_model.num_features\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.base_model.feature = nn.Sequential(\n",
    "            self.base_model.head.global_pool,\n",
    "            self.base_model.head.drop,\n",
    "        )\n",
    "        if isinstance(num_classes, list):\n",
    "            self.heads = nn.ModuleList([nn.Linear(self.num_features, num_class) for num_class in num_classes])\n",
    "        else:\n",
    "            self.head = self.base_model.head\n",
    "            self.head.fc = nn.Linear(self.num_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the base model\n",
    "        x = self.base_model.forward_features(x)\n",
    "        x = self.base_model.feature(x)\n",
    "        # print(x.shape)\n",
    "\n",
    "        if isinstance(self.num_classes, list):\n",
    "            x = [head(x) for head in self.heads]\n",
    "        else:\n",
    "            x = self.head(x)\n",
    "        return x\n",
    "\n",
    "model_name = 'swin_base_patch4_window12_384_in22k'\n",
    "\n",
    "model = MultiTaskModel(model_name, num_classes)\n",
    "\n",
    "device = \"cuda\"\n",
    "model.to(device)\n",
    "\n",
    "model = nn.DataParallel(model)\n",
    "\n",
    "def iterate(model, data_loader, device, optimizer=None, phase='test', epoch='Test'):\n",
    "    if phase == 'train':\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "    else:\n",
    "        model.eval()\n",
    "    \n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    accu_loss = torch.zeros(1).to(device)  # 累计损失\n",
    "    sample_num = 0\n",
    "    tasks = ['type', 'grade']\n",
    "    y_true, y_pred, y_prob = {}, {}, {}\n",
    "    for t in tasks:\n",
    "        y_true[t], y_pred[t], y_prob[t] = [], [], []\n",
    "    data_loader = tqdm(data_loader, file=sys.stdout)\n",
    "    for step, data in enumerate(data_loader):\n",
    "        images, labels = data\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        sample_num += images.shape[0]\n",
    "\n",
    "        outputs = model.forward(images)\n",
    "\n",
    "        losses = []\n",
    "\n",
    "        for i in range(len(outputs)):\n",
    "            task = tasks[i]\n",
    "            output = outputs[i]\n",
    "            pred = torch.max(output, dim=1)[1]\n",
    "            label = labels[:,i]\n",
    "            # print(output.shape, label.shape)\n",
    "            loss = loss_function(output, label)\n",
    "            losses.append(loss)\n",
    "\n",
    "\n",
    "            y_pred[task].extend(pred.tolist())\n",
    "            y_true[task].extend(label.to(device).tolist())\n",
    "            \n",
    "            prob = F.softmax(output, dim=1)\n",
    "            y_prob[task].extend(prob.to(device).tolist())\n",
    "        \n",
    "        loss = sum(losses) / len(losses)\n",
    "        \n",
    "        accu_loss += loss.detach()\n",
    "\n",
    "        # compute acc\n",
    "        acc = []\n",
    "        f1_macro = []\n",
    "        for task in tasks:\n",
    "            acc.append(accuracy_score(y_true[task], y_pred[task]))\n",
    "            f1_macro.append(f1_score(y_true[task], y_pred[task], average='macro'))\n",
    "\n",
    "\n",
    "        data_loader.desc = \"[{} {}] loss: {:.3f}, acc: {:.3f},{:.3f}, f1_marco: {:.3f},{:.3f}\".format(\n",
    "            phase, epoch, accu_loss.item() / (step + 1), acc[0], acc[1], f1_macro[0], f1_macro[1])\n",
    "\n",
    "        if not torch.isfinite(loss):\n",
    "            print('WARNING: non-finite loss, ending training ', loss)\n",
    "            sys.exit(1)\n",
    "        if phase == 'train':\n",
    "            loss.backward()\n",
    "            optimizer.first_step(zero_grad=True)\n",
    "            outputs = model.forward(images)\n",
    "            losses = []\n",
    "            for i in range(len(outputs)):\n",
    "                output = outputs[i]\n",
    "                loss = loss_function(output, labels[:,i])\n",
    "                losses.append(loss)\n",
    "            loss = sum(losses) / len(losses)\n",
    "            loss.backward()\n",
    "            optimizer.second_step(zero_grad=True)\n",
    "            scheduler.step()\n",
    "\n",
    "    return accu_loss.item() / (step + 1), acc, f1_macro, y_true, y_pred, y_prob\n",
    "\n",
    "weights_dir = pathlib.Path('/mnt/hd0/project_large_files/bca_grading/suqh/swin_multi_task/2023_12_13_18:12:34')\n",
    "logs_save_path = weights_dir\n",
    "if os.path.exists(logs_save_path) is False:\n",
    "    os.makedirs(logs_save_path)\n",
    "log_filename = 'independent_test_'+time.strftime(\"%Y_%m_%d_%H:%M:%S\") + '.txt'\n",
    "log_filepath = os.path.join(logs_save_path, log_filename)\n",
    "weight_path = '/mnt/hd0/project_large_files/bca_grading/suqh/swin_multi_task/2023_12_13_18:12:34/model-19.pth'\n",
    "# copy_self(weights_dir)\n",
    "model.load_state_dict(torch.load(weight_path, map_location=device))\n",
    "\n",
    "bs=12\n",
    "# 实例化训练数据集\n",
    "cohorts = ['SUQH', 'QDUH', 'SHSU', 'BRACS', 'BACH']\n",
    "datatypes = {\n",
    "    'SUQH': MyDataSet,\n",
    "    'QDUH': MyDataSet,\n",
    "    'SHSU': MyDataSet,\n",
    "    'BRACS': BRACS,\n",
    "    'BACH': BACH,\n",
    "}\n",
    "\n",
    "data_root = {\n",
    "    'SUQH': pathlib.Path('/home/yuxin/ssd_data/SUQH_10x336/test'),\n",
    "    'QDUH': pathlib.Path('/mnt/hd0/project_large_files/bcacad/patches/qingdao_test/test'),\n",
    "    'SHSU': pathlib.Path('/mnt/hd0/project_large_files/bcacad/patches/shandaer_test/test'),\n",
    "    'BRACS': pathlib.Path('/mnt/hd0/project_large_files/bcacad/patches/bracs_patch/latest_version_patch'),\n",
    "    'BACH': pathlib.Path('/mnt/hd0/project_large_files/bcacad/patches/bach_patch/Photos_patch_tif'),\n",
    "}\n",
    "\n",
    "datasets = {}\n",
    "for cohort in cohorts:\n",
    "    datasets[cohort] = datatypes[cohort](data_root[cohort], data_trans['test'])\n",
    "dataloaders = {}\n",
    "for cohort in cohorts:\n",
    "    dataloaders[cohort] = torch.utils.data.DataLoader(datasets[cohort],\n",
    "                                            batch_size=bs,\n",
    "                                            shuffle=False,\n",
    "                                            pin_memory=True,\n",
    "                                            num_workers=0,\n",
    "                                            collate_fn=datasets[cohort].collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHSU\n",
      "  0%|          | 0/379 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[test SHSU] loss: 0.799, acc: 0.831,0.541, f1_marco: 0.571,0.462: 100%|██████████| 379/379 [01:05<00:00,  5.83it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cohort='SHSU'\n",
    "print(cohort)\n",
    "\n",
    "# validate\n",
    "_,_,_,y_true,y_pred,y_prob  = iterate(model=model,\n",
    "            data_loader=dataloaders[cohort],\n",
    "            device=device,\n",
    "            epoch=cohort)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHSU        : \n",
      "[Type] [AUC] [0.95120392 0.95120392] [ACC] 0.8844884488448845 [F1-Score] [0.89348752 0.87382841] [Kappa] 0.7673609496323842 [Precision] [0.90505549 0.86079545] [Recall] [0.88221154 0.88726208]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# class_level\n",
    "Acc = {}\n",
    "F1score = {}\n",
    "Auc = {}\n",
    "Kappa = {}\n",
    "Precision = {}\n",
    "Recall = {}\n",
    "# for task in ['type', 'grade']:\n",
    "    \n",
    "#     Acc[task] = accuracy_score(y_true[task], y_pred[task])\n",
    "#     y_true_b = label_binarize(y_true[task], classes=[0, 1, 2])\n",
    "#     Auc[task] = roc_auc_score(y_true_b, y_prob[task], multi_class='ovo', average=None)\n",
    "#     Kappa[task] = cohen_kappa_score(y_true[task], y_pred[task], weights='quadratic')\n",
    "#     Precision[task], Recall[task], F1score[task], _ = precision_recall_fscore_support(y_true[task], y_pred[task])\n",
    "\n",
    "# for type\n",
    "if cohort in ['SUQH', 'QDUH', 'SHSU', 'BRACS', 'BACH']:\n",
    "    type_y_pred = y_pred['type']\n",
    "    type_y_prob = y_prob['type']\n",
    "    type_y_true = y_true['type']\n",
    "    if cohort in ['SHSU']:\n",
    "        _y_preds = []\n",
    "        _y_trues = []\n",
    "        _y_probs = []\n",
    "        for i in range(len(type_y_pred)):\n",
    "            _y_pred = type_y_pred[i]\n",
    "            _y_true = type_y_true[i]\n",
    "            _y_prob = type_y_prob[i]\n",
    "            if _y_pred > 0:\n",
    "                _y_pred -= 1\n",
    "            if _y_true > 0:\n",
    "                _y_true -= 1\n",
    "            _y_prob = [_y_prob[0]+_y_prob[1], _y_prob[2]]\n",
    "            _y_preds.append(_y_pred)\n",
    "            _y_trues.append(_y_true)\n",
    "            _y_probs.append(_y_prob)\n",
    "        type_y_pred = np.array(_y_preds)\n",
    "        type_y_true = np.array(_y_trues)\n",
    "        type_y_prob = np.array(_y_probs)\n",
    "        type_y_true_b = [[1 if i == j else 0 for j in range(2)] for i in type_y_true]\n",
    "        Auc['type'] = roc_auc_score(type_y_true_b, type_y_prob, multi_class='ovo', average=None)\n",
    "    else:\n",
    "        type_y_true_b = label_binarize(type_y_true, classes=[0, 1, 2])\n",
    "        Auc['type'] = roc_auc_score(type_y_true_b, type_y_prob, multi_class='ovo', average=None)\n",
    "    Acc['type'] = accuracy_score(type_y_true, type_y_pred)\n",
    "    \n",
    "    Kappa['type'] = cohen_kappa_score(type_y_true, type_y_pred, weights='quadratic')\n",
    "    Precision['type'], Recall['type'], F1score['type'], _ = precision_recall_fscore_support(type_y_true, type_y_pred)\n",
    "    log_txt_formatter = \"{cohort}: \\n\" \\\n",
    "        \"[Type] [AUC] {auc_t} [ACC] {acc_t} [F1-Score] {f1_t} [Kappa] {kappt_t} [Precision] {p_t} [Recall] {r_t}\\n\\n\"\n",
    "    to_write = log_txt_formatter.format(cohort=\"{: <12}\".format(cohort),\n",
    "                                        acc_t = Acc['type'],\n",
    "                                        f1_t = F1score['type'],\n",
    "                                        auc_t = Auc['type'],\n",
    "                                        kappt_t = Kappa['type'],\n",
    "                                        p_t = Precision['type'],\n",
    "                                        r_t = Recall['type'],\n",
    "                                    )\n",
    "    print(to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHSU        : \n",
      "[Grade] [AUC] [0.68611167 0.51540354 0.68227978] [ACC] 0.4639423076923077 [F1-Score] [0.14860681 0.53760977 0.4701795 ] [Kappa] 0.22556666197381392 [Precision] [0.12060302 0.47664184 0.65378422] [Recall] [0.19354839 0.61646235 0.36708861]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if cohort in ['SUQH', 'QDUH', 'SHSU']:\n",
    "    tumor_idx = np.where(np.array(y_true['type']) != 0)[0]\n",
    "    grade_y_pred = np.array(y_pred['grade'])[tumor_idx]\n",
    "    grade_y_prob = np.array(y_prob['grade'])[tumor_idx]\n",
    "    grade_y_true = np.array(y_true['grade'])[tumor_idx]\n",
    "    Acc['grade'] = accuracy_score(grade_y_true, grade_y_pred)\n",
    "    grade_y_true_b = label_binarize(grade_y_true, classes=[0, 1, 2])\n",
    "    Auc['grade'] = roc_auc_score(grade_y_true_b, grade_y_prob, multi_class='ovo', average=None)\n",
    "    Kappa['grade'] = cohen_kappa_score(grade_y_true, grade_y_pred, weights='quadratic')\n",
    "    Precision['grade'], Recall['grade'], F1score['grade'], _ = precision_recall_fscore_support(grade_y_true, grade_y_pred)\n",
    "\n",
    "    log_txt_formatter = \"{cohort}: \\n\" \\\n",
    "        \"[Grade] [AUC] {auc_g} [ACC] {acc_g} [F1-Score] {f1_g} [Kappa] {kappt_g} [Precision] {p_g} [Recall] {r_g}\\n\\n\"\n",
    "    to_write = log_txt_formatter.format(cohort=\"{: <12}\".format(cohort),\n",
    "                                        acc_g = Acc['grade'],\n",
    "                                        f1_g = F1score['grade'],\n",
    "                                        auc_g = Auc['grade'],\n",
    "                                        kappt_g = Kappa['grade'],\n",
    "                                        p_g = Precision['grade'],\n",
    "                                        r_g = Recall['grade'],    \n",
    "                                    )\n",
    "    print(to_write)\n",
    "    # with open(log_filepath, \"a\") as f:\n",
    "        # f.write(to_write)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmpretrain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
