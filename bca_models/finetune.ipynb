{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from rl_benchmarks.models import iBOTViT\n",
    "from openslide import open_slide\n",
    "from openslide.deepzoom import DeepZoomGenerator\n",
    "import pathlib\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from multiprocessing import Pool\n",
    "import umap\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from rl_benchmarks.utils.linear_evaluation import get_binary_class_metrics, get_bootstrapped_metrics\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_recall_fscore_support, roc_auc_score\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-10-24 17:35:57.754\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrl_benchmarks.models.feature_extractors.ibot_vit\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m78\u001b[0m - \u001b[1mPretrained weights found at /home/yuxin/Downloads/ibot_vit_base_pancan.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['masked_embed', 'head.mlp.0.weight', 'head.mlp.0.bias', 'head.mlp.2.weight', 'head.mlp.2.bias', 'head.mlp.4.weight', 'head.mlp.4.bias', 'head.last_layer.weight_g', 'head.last_layer.weight_v', 'head.last_layer2.weight_g', 'head.last_layer2.weight_v'])\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "weights_path = '/home/yuxin/Downloads/ibot_vit_base_pancan.pth'\n",
    "ibot_base_pancancer = iBOTViT(architecture=\"vit_base_pancan\", encoder=\"student\", weights_path=weights_path).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 224\n",
    "data_root = pathlib.Path('/mnt/hd0/original_datasets/JT_Breast/SUQH/40xTiles')\n",
    "split_root = pathlib.Path('/mnt/hd0/project_large_files/bca_grading/suqh/split_data')\n",
    "phases = ['train', 'val', 'test']\n",
    "save_root = pathlib.Path('/mnt/hd0/project_large_files/bca_grading/suqh/histosslscaling_finetune')\n",
    "save_root.mkdir(exist_ok=True)\n",
    "data_trans = dict(\n",
    "    test = transforms.Compose([\n",
    "                                transforms.CenterCrop(patch_size*4),\n",
    "                                transforms.Resize(patch_size),\n",
    "                                ibot_base_pancancer.transform,\n",
    "                                ]),\n",
    "    train = transforms.Compose([\n",
    "                                transforms.CenterCrop(patch_size*4),\n",
    "                                transforms.Resize(patch_size),\n",
    "                                transforms.RandomHorizontalFlip(),\n",
    "                                ibot_base_pancancer.transform,\n",
    "                                ]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataSet(Dataset):\n",
    "    \"\"\"自定义数据集\"\"\"\n",
    "\n",
    "    # def __init__(self, images_path: list, images_class: list, images_ncl: list, images_epi: list, images_tub: list, images_mit: list, transform=None):\n",
    "    def __init__(self, data_root:str, split_file:str,transform=None):\n",
    "        images_path = []\n",
    "        images_class = []\n",
    "        with open(split_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                label = line[-2:-1]\n",
    "                if label == '0' or label == '2':\n",
    "                    label = 0\n",
    "                else:\n",
    "                    label = 1\n",
    "                img_name = line[:-3]\n",
    "                img_path = os.path.join(data_root, img_name)\n",
    "                images_path.append(img_path)\n",
    "                images_class.append(int(label))\n",
    " \n",
    "        self.images_path = images_path\n",
    "        self.images_class = images_class\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_path)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        img = Image.open(self.images_path[item])\n",
    "        # RGB为彩色图片，L为灰度图片\n",
    "        if img.mode != 'RGB':\n",
    "            raise ValueError(\"image: {} isn't RGB mode.\".format(self.images_path[item]))\n",
    "        label = self.images_class[item]\n",
    "\n",
    "        # img_full, img_name = os.path.split(self.images_path[item])\n",
    "        # img_n, ext = os.path.splitext(img_name)\n",
    "\n",
    "        seed = np.random.randint(2147483647)\n",
    "        if self.transform is not None:\n",
    "            torch.manual_seed(seed)\n",
    "            torch.cuda.manual_seed(seed)\n",
    "            img = self.transform(img)\n",
    "\n",
    "\n",
    "        # img.save(os.path.join('./img_out',img_n+'_1'+'.jpg'))\n",
    "        # ncl.save(os.path.join('./img_out', img_n + '_2' + '.jpg'))\n",
    "        # epi.save(os.path.join('./img_out', img_n + '_3' + '.jpg'))\n",
    "        # tub.save(os.path.join('./img_out', img_n + '_4' + '.jpg'))\n",
    "        # mit.save(os.path.join('./img_out', img_n + '_5' + '.jpg'))\n",
    "\n",
    "        return img, label\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        # 官方实现的default_collate可以参考\n",
    "        # https://github.com/pytorch/pytorch/blob/67b7e751e6b5931a9f45274653f4f653a4e6cdf6/torch/utils/data/_utils/collate.py\n",
    "        images, labels = tuple(zip(*batch))\n",
    "\n",
    "        images = torch.stack(images, dim=0)\n",
    "        labels = torch.as_tensor(labels)\n",
    "\n",
    "        return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training head.weight\n",
      "training head.bias\n"
     ]
    }
   ],
   "source": [
    "num_classes = 2\n",
    "ibot_base_pancancer.head = torch.nn.Linear(768, num_classes).to(device)\n",
    "ibot_base_pancancer.head.weight.data.normal_(mean=0.0, std=0.02)\n",
    "ibot_base_pancancer.head.bias.data.zero_()\n",
    "model = ibot_base_pancancer\n",
    "\n",
    "for name, para in model.named_parameters():\n",
    "    if \"head\" not in name:\n",
    "        para.requires_grad_(False)\n",
    "    else:\n",
    "        para.requires_grad_(True)\n",
    "        print(\"training {}\".format(name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 8 dataloader workers every process\n"
     ]
    }
   ],
   "source": [
    "bs=14\n",
    "# 实例化训练数据集\n",
    "train_dataset = MyDataSet(data_root=data_root,\n",
    "                            split_file=split_root / 'train.txt',\n",
    "                            transform=data_trans[\"train\"])\n",
    "\n",
    "# 实例化验证数据集\n",
    "val_dataset = MyDataSet(data_root=data_root,\n",
    "                        split_file=split_root / 'val.txt',\n",
    "                        transform=data_trans[\"test\"])\n",
    "\n",
    "batch_size = bs\n",
    "nw = min([os.cpu_count(), batch_size if batch_size > 1 else 0, 8])  # number of workers\n",
    "print('Using {} dataloader workers every process'.format(nw))\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True,\n",
    "                                            pin_memory=True,\n",
    "                                            num_workers=nw,\n",
    "                                            collate_fn=train_dataset.collate_fn)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=False,\n",
    "                                            pin_memory=True,\n",
    "                                            num_workers=nw,\n",
    "                                            collate_fn=val_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = optim.AdamW(pg, lr=1e-4, weight_decay=5E-2)\n",
    "best_loss = np.inf\n",
    "epochs_without_improvement = 0\n",
    "patience = 7\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, data_loader, device, epoch):\n",
    "    model.train()\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    accu_loss = torch.zeros(1).to(device)  # 累计损失\n",
    "    accu_num = torch.zeros(1).to(device)   # 累计预测正确的样本数\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    sample_num = 0\n",
    "    y_true, y_pred, y_score_auc = [], [], []\n",
    "    data_loader = tqdm(data_loader, file=sys.stdout)\n",
    "    for step, data in enumerate(data_loader):\n",
    "        # images, labels, ncl, epi, tub, mit = data\n",
    "        images, labels = data\n",
    "        sample_num += images.shape[0]\n",
    "\n",
    "        pred = model.forward(images.to(device))\n",
    "        pred_classes = torch.max(pred, dim=1)[1]\n",
    "        accu_num += torch.eq(pred_classes, labels.to(device)).sum()\n",
    "\n",
    "        loss = loss_function(pred, labels.to(device))\n",
    "        # print(loss, pred.shape, labels)\n",
    "        loss.backward()\n",
    "        accu_loss += loss.detach()\n",
    "\n",
    "        #################################\n",
    "        # print(pred_classes.tolist())\n",
    "        y_pred.extend(pred_classes.tolist())\n",
    "        # print(y_pred)\n",
    "        # print(type(y_pred))\n",
    "\n",
    "        # print(labels.to(device).tolist())\n",
    "        y_true.extend(labels.to(device).tolist())\n",
    "        # print(y_true)\n",
    "        # print(type(y_true))\n",
    "        f1_macro=f1_score(y_true, y_pred, average='macro')\n",
    "        f1_micro = f1_score(y_true, y_pred, average='micro')\n",
    "        ##auc\n",
    "        pred_auc = F.softmax(pred, dim=1)\n",
    "        y_score_auc.extend(pred_auc.to(device).tolist())\n",
    "        ################end################\n",
    "\n",
    "        data_loader.desc = \"[train epoch {}] loss: {:.3f}, acc: {:.3f}, f1_a: {:.3f}, f1_i: {:.3f}\".format(epoch,\n",
    "                                                                                                           accu_loss.item() / (step + 1),\n",
    "                                                                                                           accu_num.item() / sample_num,\n",
    "                                                                                                           f1_macro,\n",
    "                                                                                                           f1_micro)\n",
    "\n",
    "        if not torch.isfinite(loss):\n",
    "            print('WARNING: non-finite loss, ending training ', loss)\n",
    "            sys.exit(1)\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        # scheduler.step()\n",
    "        # break\n",
    "\n",
    "    # y_true2 = label_binarize(y_true, classes=[0, 1, 2])\n",
    "    # auc_class = roc_auc_score(y_true2, y_score_auc, multi_class=\"ovo\", average=None)\n",
    "    # auc1 = roc_auc_score(y_true2, y_score_auc, multi_class=\"ovo\", average=\"macro\")\n",
    "    \n",
    "    y_score_auc = np.array(y_score_auc)\n",
    "    y_true = np.array(y_true)\n",
    "    # print(y_true)\n",
    "    # print(y_score_auc)\n",
    "    # print(y_true.shape, y_score_auc.shape)\n",
    "\n",
    "    \n",
    "\n",
    "    auc_score = roc_auc_score(y_true, y_score_auc[:,1])\n",
    "    auc1 = auc_class = auc_score\n",
    "\n",
    "    return accu_loss.item() / (step + 1), accu_num.item() / sample_num, auc1, f1_macro, f1_micro, auc_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, data_loader, device, epoch):\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    accu_num = torch.zeros(1).to(device)   # 累计预测正确的样本数\n",
    "    accu_loss = torch.zeros(1).to(device)  # 累计损失\n",
    "\n",
    "    sample_num = 0\n",
    "    y_true, y_pred, y_score_auc = [], [], []\n",
    "    data_loader = tqdm(data_loader, file=sys.stdout)\n",
    "    for step, data in enumerate(data_loader):\n",
    "        images, labels = data\n",
    "        sample_num += images.shape[0]\n",
    "\n",
    "        pred = model.forward(images.to(device))\n",
    "        pred_classes = torch.max(pred, dim=1)[1]\n",
    "        accu_num += torch.eq(pred_classes, labels.to(device)).sum()\n",
    "\n",
    "\n",
    "\n",
    "        loss = loss_function(pred, labels.to(device))\n",
    "        # print(loss)\n",
    "        accu_loss += loss\n",
    "\n",
    "        ################################\n",
    "        # print(pred_classes.tolist())\n",
    "        y_pred.extend(pred_classes.tolist())\n",
    "        # print(y_pred)\n",
    "        # print(type(y_pred))\n",
    "        y_true.extend(labels.to(device).tolist())\n",
    "        # print(y_true)\n",
    "        # print(type(y_true))\n",
    "        f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "        f1_micro = f1_score(y_true, y_pred, average='micro')\n",
    "        ##auc\n",
    "        pred_auc = F.softmax(pred, dim=1)\n",
    "        y_score_auc.extend(pred_auc.to(device).tolist())\n",
    "        ################end################\n",
    "\n",
    "        data_loader.desc = \"[valid epoch {}] loss: {:.3f}, acc: {:.3f}, f1_a: {:.3f}, f1_i: {:.3f}\".format(epoch,\n",
    "                                                                                                           accu_loss.item() / (\n",
    "                                                                                                                       step + 1),\n",
    "                                                                                                           accu_num.item() / sample_num,\n",
    "                                                                                                           f1_macro,\n",
    "                                                                                                           f1_micro)\n",
    "        # break\n",
    "\n",
    "    # y_true2 = label_binarize(y_true, classes=[0, 1, 2])\n",
    "    # auc_class = roc_auc_score(y_true2, y_score_auc, multi_class=\"ovo\", average=None)\n",
    "    # auc1 = roc_auc_score(y_true2, y_score_auc, multi_class=\"ovo\", average=\"macro\")\n",
    "    y_score_auc = np.array(y_score_auc)\n",
    "    y_true = np.array(y_true)\n",
    "    auc_score = roc_auc_score(y_true, y_score_auc[:,1])\n",
    "    auc1 = auc_class = auc_score\n",
    "\n",
    "    return accu_loss.item() / (step + 1), accu_num.item() / sample_num, auc1, f1_macro, f1_micro, auc_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train epoch 0] loss: 0.710, acc: 0.643, f1_a: 0.591, f1_i: 0.643:   0%|          | 0/250 [00:11<?, ?it/s]\n",
      "[valid epoch 0] loss: 0.618, acc: 0.786, f1_a: 0.754, f1_i: 0.786:   0%|          | 0/62 [00:06<?, ?it/s]\n",
      "[train epoch 1] loss: 0.707, acc: 0.429, f1_a: 0.417, f1_i: 0.429:   0%|          | 0/250 [00:11<?, ?it/s]\n",
      "[valid epoch 1] loss: 0.619, acc: 0.786, f1_a: 0.754, f1_i: 0.786:   0%|          | 0/62 [00:06<?, ?it/s]\n",
      "[train epoch 2] loss: 0.629, acc: 0.643, f1_a: 0.626, f1_i: 0.643:   0%|          | 0/250 [00:07<?, ?it/s]\n",
      "[valid epoch 2] loss: 0.619, acc: 0.786, f1_a: 0.754, f1_i: 0.786:   0%|          | 0/62 [00:05<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "log_time = time.strftime(\"%Y_%m_%d_%H:%M:%S\")\n",
    "log_filename = 'train_'+ log_time + '.txt'\n",
    "log_filepath = save_root / log_filename\n",
    "weight_save_path = save_root / log_time\n",
    "\n",
    "\n",
    "tb_writer = SummaryWriter()\n",
    "for epoch in range(epochs):\n",
    "        # train\n",
    "        train_loss, train_acc, train_auc, train_f1_macro, train_f1_micro, train_auc_class = train_one_epoch(model=model,\n",
    "                                                                                                            optimizer=optimizer,\n",
    "                                                                                                            data_loader=train_loader,\n",
    "                                                                                                            device=device,\n",
    "                                                                                                            epoch=epoch,)\n",
    "\n",
    "        # validate\n",
    "        val_loss, val_acc, val_auc, val_f1_macro,val_f1_micro, val_auc_class= evaluate(model=model,\n",
    "                                                                                       data_loader=val_loader,\n",
    "                                                                                       device=device,\n",
    "                                                                                       epoch=epoch)\n",
    "\n",
    "        tags = [\"train_loss\", \"train_acc\", \"val_loss\", \"val_acc\", \"learning_rate\"]\n",
    "        tb_writer.add_scalar(tags[0], train_loss, epoch)\n",
    "        tb_writer.add_scalar(tags[1], train_acc, epoch)\n",
    "        tb_writer.add_scalar(tags[2], val_loss, epoch)\n",
    "        tb_writer.add_scalar(tags[3], val_acc, epoch)\n",
    "        tb_writer.add_scalar(tags[4], optimizer.param_groups[0][\"lr\"], epoch)\n",
    "\n",
    "        # write to txt\n",
    "        log_txt_formatter = \"[train Epoch] {epoch:03d} [Loss] {train_loss} [auc] {train_auc} [acc] {train_acc} [f1_macro] {train_f1_ma} [f1_micro] {train_f1_mi} [AUC] {train_auc_c}\\n\" \\\n",
    "                            \"[valid Epoch] {epoch:03d} [Loss] {valid_loss} [auc] {valid_auc} [acc] {valid_acc} [f1_macro] {valid_f1_ma} [f1_micro] {valid_f1_mi} [AUC] {valid_auc_c}\\n\"\n",
    "\n",
    "        to_write = log_txt_formatter.format(epoch=epoch,\n",
    "                                            train_loss=\" \".join([\"{}\".format('%.3f' % train_loss)]),\n",
    "                                            train_auc=\" \".join([\"{}\".format('%.3f' % train_auc)]),\n",
    "                                            train_acc=\" \".join([\"{}\".format('%.3f' % train_acc)]),\n",
    "                                            train_f1_ma=\" \".join([\"{}\".format('%.3f' % train_f1_macro)]),\n",
    "                                            train_f1_mi=\" \".join([\"{}\".format('%.3f' % train_f1_micro)]),\n",
    "                                            train_auc_c=train_auc_class,\n",
    "                                            # epoch=epoch,\n",
    "                                            valid_loss=\" \".join([\"{}\".format('%.3f' % val_loss)]),\n",
    "                                            valid_auc=\" \".join([\"{}\".format('%.3f' % val_auc)]),\n",
    "                                            valid_acc=\" \".join([\"{}\".format('%.3f' % val_acc)]),\n",
    "                                            valid_f1_ma=\" \".join([\"{}\".format('%.3f' % val_f1_macro)]),\n",
    "                                            valid_f1_mi=\" \".join([\"{}\".format('%.3f' % val_f1_micro)]),\n",
    "                                            valid_auc_c=val_auc_class,)\n",
    "        save_root.mkdir(exist_ok=True)\n",
    "        weight_save_path.mkdir(exist_ok=True)\n",
    "\n",
    "        with open(log_filepath, \"a\") as f:\n",
    "            f.write(to_write)\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "            torch.save(model.state_dict(), os.path.join(weight_save_path, 'model-{}.pth').format(epoch))\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement == patience:\n",
    "            print('Early stopping at epoch {}...'.format(epoch+1))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iBOTViT(\n",
       "  (feature_extractor): VisionTransformer(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (blocks): ModuleList(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (head): Identity()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ibot_base_pancancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_extractor.cls_token True\n",
      "feature_extractor.pos_embed True\n",
      "feature_extractor.patch_embed.proj.weight True\n",
      "feature_extractor.patch_embed.proj.bias True\n",
      "feature_extractor.blocks.0.norm1.weight True\n",
      "feature_extractor.blocks.0.norm1.bias True\n",
      "feature_extractor.blocks.0.attn.qkv.weight True\n",
      "feature_extractor.blocks.0.attn.qkv.bias True\n",
      "feature_extractor.blocks.0.attn.proj.weight True\n",
      "feature_extractor.blocks.0.attn.proj.bias True\n",
      "feature_extractor.blocks.0.norm2.weight True\n",
      "feature_extractor.blocks.0.norm2.bias True\n",
      "feature_extractor.blocks.0.mlp.fc1.weight True\n",
      "feature_extractor.blocks.0.mlp.fc1.bias True\n",
      "feature_extractor.blocks.0.mlp.fc2.weight True\n",
      "feature_extractor.blocks.0.mlp.fc2.bias True\n",
      "feature_extractor.blocks.1.norm1.weight True\n",
      "feature_extractor.blocks.1.norm1.bias True\n",
      "feature_extractor.blocks.1.attn.qkv.weight True\n",
      "feature_extractor.blocks.1.attn.qkv.bias True\n",
      "feature_extractor.blocks.1.attn.proj.weight True\n",
      "feature_extractor.blocks.1.attn.proj.bias True\n",
      "feature_extractor.blocks.1.norm2.weight True\n",
      "feature_extractor.blocks.1.norm2.bias True\n",
      "feature_extractor.blocks.1.mlp.fc1.weight True\n",
      "feature_extractor.blocks.1.mlp.fc1.bias True\n",
      "feature_extractor.blocks.1.mlp.fc2.weight True\n",
      "feature_extractor.blocks.1.mlp.fc2.bias True\n",
      "feature_extractor.blocks.2.norm1.weight True\n",
      "feature_extractor.blocks.2.norm1.bias True\n",
      "feature_extractor.blocks.2.attn.qkv.weight True\n",
      "feature_extractor.blocks.2.attn.qkv.bias True\n",
      "feature_extractor.blocks.2.attn.proj.weight True\n",
      "feature_extractor.blocks.2.attn.proj.bias True\n",
      "feature_extractor.blocks.2.norm2.weight True\n",
      "feature_extractor.blocks.2.norm2.bias True\n",
      "feature_extractor.blocks.2.mlp.fc1.weight True\n",
      "feature_extractor.blocks.2.mlp.fc1.bias True\n",
      "feature_extractor.blocks.2.mlp.fc2.weight True\n",
      "feature_extractor.blocks.2.mlp.fc2.bias True\n",
      "feature_extractor.blocks.3.norm1.weight True\n",
      "feature_extractor.blocks.3.norm1.bias True\n",
      "feature_extractor.blocks.3.attn.qkv.weight True\n",
      "feature_extractor.blocks.3.attn.qkv.bias True\n",
      "feature_extractor.blocks.3.attn.proj.weight True\n",
      "feature_extractor.blocks.3.attn.proj.bias True\n",
      "feature_extractor.blocks.3.norm2.weight True\n",
      "feature_extractor.blocks.3.norm2.bias True\n",
      "feature_extractor.blocks.3.mlp.fc1.weight True\n",
      "feature_extractor.blocks.3.mlp.fc1.bias True\n",
      "feature_extractor.blocks.3.mlp.fc2.weight True\n",
      "feature_extractor.blocks.3.mlp.fc2.bias True\n",
      "feature_extractor.blocks.4.norm1.weight True\n",
      "feature_extractor.blocks.4.norm1.bias True\n",
      "feature_extractor.blocks.4.attn.qkv.weight True\n",
      "feature_extractor.blocks.4.attn.qkv.bias True\n",
      "feature_extractor.blocks.4.attn.proj.weight True\n",
      "feature_extractor.blocks.4.attn.proj.bias True\n",
      "feature_extractor.blocks.4.norm2.weight True\n",
      "feature_extractor.blocks.4.norm2.bias True\n",
      "feature_extractor.blocks.4.mlp.fc1.weight True\n",
      "feature_extractor.blocks.4.mlp.fc1.bias True\n",
      "feature_extractor.blocks.4.mlp.fc2.weight True\n",
      "feature_extractor.blocks.4.mlp.fc2.bias True\n",
      "feature_extractor.blocks.5.norm1.weight True\n",
      "feature_extractor.blocks.5.norm1.bias True\n",
      "feature_extractor.blocks.5.attn.qkv.weight True\n",
      "feature_extractor.blocks.5.attn.qkv.bias True\n",
      "feature_extractor.blocks.5.attn.proj.weight True\n",
      "feature_extractor.blocks.5.attn.proj.bias True\n",
      "feature_extractor.blocks.5.norm2.weight True\n",
      "feature_extractor.blocks.5.norm2.bias True\n",
      "feature_extractor.blocks.5.mlp.fc1.weight True\n",
      "feature_extractor.blocks.5.mlp.fc1.bias True\n",
      "feature_extractor.blocks.5.mlp.fc2.weight True\n",
      "feature_extractor.blocks.5.mlp.fc2.bias True\n",
      "feature_extractor.blocks.6.norm1.weight True\n",
      "feature_extractor.blocks.6.norm1.bias True\n",
      "feature_extractor.blocks.6.attn.qkv.weight True\n",
      "feature_extractor.blocks.6.attn.qkv.bias True\n",
      "feature_extractor.blocks.6.attn.proj.weight True\n",
      "feature_extractor.blocks.6.attn.proj.bias True\n",
      "feature_extractor.blocks.6.norm2.weight True\n",
      "feature_extractor.blocks.6.norm2.bias True\n",
      "feature_extractor.blocks.6.mlp.fc1.weight True\n",
      "feature_extractor.blocks.6.mlp.fc1.bias True\n",
      "feature_extractor.blocks.6.mlp.fc2.weight True\n",
      "feature_extractor.blocks.6.mlp.fc2.bias True\n",
      "feature_extractor.blocks.7.norm1.weight True\n",
      "feature_extractor.blocks.7.norm1.bias True\n",
      "feature_extractor.blocks.7.attn.qkv.weight True\n",
      "feature_extractor.blocks.7.attn.qkv.bias True\n",
      "feature_extractor.blocks.7.attn.proj.weight True\n",
      "feature_extractor.blocks.7.attn.proj.bias True\n",
      "feature_extractor.blocks.7.norm2.weight True\n",
      "feature_extractor.blocks.7.norm2.bias True\n",
      "feature_extractor.blocks.7.mlp.fc1.weight True\n",
      "feature_extractor.blocks.7.mlp.fc1.bias True\n",
      "feature_extractor.blocks.7.mlp.fc2.weight True\n",
      "feature_extractor.blocks.7.mlp.fc2.bias True\n",
      "feature_extractor.blocks.8.norm1.weight True\n",
      "feature_extractor.blocks.8.norm1.bias True\n",
      "feature_extractor.blocks.8.attn.qkv.weight True\n",
      "feature_extractor.blocks.8.attn.qkv.bias True\n",
      "feature_extractor.blocks.8.attn.proj.weight True\n",
      "feature_extractor.blocks.8.attn.proj.bias True\n",
      "feature_extractor.blocks.8.norm2.weight True\n",
      "feature_extractor.blocks.8.norm2.bias True\n",
      "feature_extractor.blocks.8.mlp.fc1.weight True\n",
      "feature_extractor.blocks.8.mlp.fc1.bias True\n",
      "feature_extractor.blocks.8.mlp.fc2.weight True\n",
      "feature_extractor.blocks.8.mlp.fc2.bias True\n",
      "feature_extractor.blocks.9.norm1.weight True\n",
      "feature_extractor.blocks.9.norm1.bias True\n",
      "feature_extractor.blocks.9.attn.qkv.weight True\n",
      "feature_extractor.blocks.9.attn.qkv.bias True\n",
      "feature_extractor.blocks.9.attn.proj.weight True\n",
      "feature_extractor.blocks.9.attn.proj.bias True\n",
      "feature_extractor.blocks.9.norm2.weight True\n",
      "feature_extractor.blocks.9.norm2.bias True\n",
      "feature_extractor.blocks.9.mlp.fc1.weight True\n",
      "feature_extractor.blocks.9.mlp.fc1.bias True\n",
      "feature_extractor.blocks.9.mlp.fc2.weight True\n",
      "feature_extractor.blocks.9.mlp.fc2.bias True\n",
      "feature_extractor.blocks.10.norm1.weight True\n",
      "feature_extractor.blocks.10.norm1.bias True\n",
      "feature_extractor.blocks.10.attn.qkv.weight True\n",
      "feature_extractor.blocks.10.attn.qkv.bias True\n",
      "feature_extractor.blocks.10.attn.proj.weight True\n",
      "feature_extractor.blocks.10.attn.proj.bias True\n",
      "feature_extractor.blocks.10.norm2.weight True\n",
      "feature_extractor.blocks.10.norm2.bias True\n",
      "feature_extractor.blocks.10.mlp.fc1.weight True\n",
      "feature_extractor.blocks.10.mlp.fc1.bias True\n",
      "feature_extractor.blocks.10.mlp.fc2.weight True\n",
      "feature_extractor.blocks.10.mlp.fc2.bias True\n",
      "feature_extractor.blocks.11.norm1.weight True\n",
      "feature_extractor.blocks.11.norm1.bias True\n",
      "feature_extractor.blocks.11.attn.qkv.weight True\n",
      "feature_extractor.blocks.11.attn.qkv.bias True\n",
      "feature_extractor.blocks.11.attn.proj.weight True\n",
      "feature_extractor.blocks.11.attn.proj.bias True\n",
      "feature_extractor.blocks.11.norm2.weight True\n",
      "feature_extractor.blocks.11.norm2.bias True\n",
      "feature_extractor.blocks.11.mlp.fc1.weight True\n",
      "feature_extractor.blocks.11.mlp.fc1.bias True\n",
      "feature_extractor.blocks.11.mlp.fc2.weight True\n",
      "feature_extractor.blocks.11.mlp.fc2.bias True\n",
      "feature_extractor.norm.weight True\n",
      "feature_extractor.norm.bias True\n"
     ]
    }
   ],
   "source": [
    "for name, p  in ibot_base_pancancer.named_parameters():\n",
    "    print(name, p.requires_grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "histosslscaling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
